{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started\n",
      "Model loaded\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Initialize RISE explainer\u001b[39;00m\n\u001b[0;32m     46\u001b[0m explainer \u001b[38;5;241m=\u001b[39m RISE(model, (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n\u001b[1;32m---> 47\u001b[0m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmask.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# This should load masks with shape (19, 10, 1, 224, 224)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRISE explainer initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Set up metrics\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\Causal-AI\\explanations.py:44\u001b[0m, in \u001b[0;36mRISE.load_masks\u001b[1;34m(self, filepath, p1, s)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, filepath, p1, s):\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not int"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming the path to your RISE-SFL folder is set up correctly\n",
    "rise_sfl_path = r\"C:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\RISEX\"\n",
    "sys.path.append(rise_sfl_path)\n",
    "# Assuming these imports work as in your original script\n",
    "from explanations import RISE\n",
    "from utils import read_tensor, get_class_name, tensor_imshow\n",
    "\n",
    "rise_sfl_path = r\"C:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\metrics-saliency-maps\"  # Replace with the actual path to your RISE-SFL folder\n",
    "sys.path.append(rise_sfl_path)\n",
    "#saliency_maps = extract_specific_saliency_maps(pixel_datasets[0])\n",
    "from saliency_maps_metrics.multi_step_metrics import Deletion, Insertion, compute_correlation, compute_auc_metric\n",
    "from saliency_maps_metrics.data_replace import select_data_replace_method\n",
    "\n",
    "\n",
    "# Make sure this import works with your file structure\n",
    "\n",
    "print(\"Script started\")\n",
    "\n",
    "# Set up CUDA\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Load the model\n",
    "model = models.resnet50(True)\n",
    "model = nn.Sequential(model, nn.Softmax(dim=1))\n",
    "model = model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Model loaded\")\n",
    "\n",
    "N=200\n",
    "s = 8\n",
    "p1 = 0.2\n",
    "# Initialize RISE explainer\n",
    "explainer = RISE(model, (224, 224))\n",
    "explainer.load_masks(200, 0.2,'mask.npy')  # This should load masks with shape (19, 10, 1, 224, 224)\n",
    "\n",
    "print(\"RISE explainer initialized\")\n",
    "\n",
    "# Set up metrics\n",
    "deletion_metric = Deletion(data_replace_method=\"black\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\n",
    "insertion_metric = Insertion(data_replace_method=\"blur\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\n",
    "\n",
    "# Process all images in the folder\n",
    "image_folder = r'C:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\RISE-SFL\\data_demo'\n",
    "\n",
    "print(f\"Checking image folder: {image_folder}\")\n",
    "print(f\"Folder exists: {os.path.exists(image_folder)}\")\n",
    "print(f\"Folder contents: {os.listdir(image_folder)}\")\n",
    "\n",
    "image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "print(f\"Found {len(image_files)} images in the folder\")\n",
    "print(f\"Image files: {image_files}\")\n",
    "\n",
    "# Function to calculate sparsity\n",
    "def calculate_sparsity(saliency_map):\n",
    "    max_value = np.max(saliency_map)\n",
    "    mean_value = np.mean(saliency_map)\n",
    "    return max_value / mean_value if mean_value != 0 else float('inf')\n",
    "\n",
    "# Initialize dictionary to store cumulative results\n",
    "cumulative_results = {\n",
    "    \"RISE\": {\"DAUC\": 0, \"DC\": 0, \"IAUC\": 0, \"IC\": 0, \"Sparsity\": 0}\n",
    "}\n",
    "\n",
    "# Process each image\n",
    "for image_index, image_file in enumerate(tqdm(image_files, desc=\"Processing images\")):\n",
    "    img_path = os.path.join(image_folder, image_file)\n",
    "    img = read_tensor(img_path).cuda()\n",
    "\n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "    target_class = output.argmax().item()\n",
    "\n",
    "    # Generate saliency map\n",
    "    sal_og = explainer(img, image_index=image_index)[target_class].cpu().numpy()\n",
    "\n",
    "    # Calculate metrics\n",
    "    saliency_map_tensor = torch.from_numpy(sal_og).float().unsqueeze(0).unsqueeze(0).cuda()\n",
    "\n",
    "    # Compute Deletion metrics\n",
    "    deletion_result = deletion_metric(model, img, saliency_map_tensor, class_to_explain_list=[target_class])\n",
    "    cumulative_results[\"RISE\"][\"DAUC\"] += deletion_result[\"dauc\"]\n",
    "    cumulative_results[\"RISE\"][\"DC\"] += deletion_result[\"dc\"]\n",
    "\n",
    "    # Compute Insertion metrics\n",
    "    insertion_result = insertion_metric(model, img, saliency_map_tensor, class_to_explain_list=[target_class])\n",
    "    cumulative_results[\"RISE\"][\"IAUC\"] += insertion_result[\"iauc\"]\n",
    "    cumulative_results[\"RISE\"][\"IC\"] += insertion_result[\"ic\"]\n",
    "\n",
    "    # Calculate sparsity\n",
    "    cumulative_results[\"RISE\"][\"Sparsity\"] += calculate_sparsity(sal_og)\n",
    "\n",
    "    # Visualize and save saliency map\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    tensor_imshow(img[0].cpu())\n",
    "    plt.title(f\"Original - {get_class_name(target_class)}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    tensor_imshow(img[0].cpu())\n",
    "    plt.imshow(sal_og, cmap='jet', alpha=0.5)\n",
    "    plt.title(f\"Saliency Map (Image {image_index + 1})\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.savefig(f\"saliency_{image_file}_image{image_index + 1}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Calculate averages\n",
    "num_images = len(image_files)\n",
    "results = []\n",
    "for name, metrics in cumulative_results.items():\n",
    "    avg_metrics = [name] + [metrics[key] / num_images for key in [\"DAUC\", \"DC\", \"IAUC\", \"IC\", \"Sparsity\"]]\n",
    "    results.append(avg_metrics)\n",
    "\n",
    "# Create a table using tabulate\n",
    "headers = [\"Saliency Map\", \"Avg DAUC\", \"Avg DC\", \"Avg IAUC\", \"Avg IC\", \"Avg Sparsity\"]\n",
    "table = tabulate(results, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\")\n",
    "\n",
    "# Print the table\n",
    "print(f\"\\nAverage Metrics for {num_images} images:\")\n",
    "print(table)\n",
    "\n",
    "print(\"Script completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RISE ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started\n",
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating filters: 100%|██████████| 100/100 [00:00<00:00, 888.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RISE explainer initialized\n",
      "Checking image folder: C:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\RISE-SFL\\data_demo\n",
      "Folder exists: True\n",
      "Folder contents: ['.DS_Store', 'catdog.png', 'golden-retriever-puppy.jpg', 'goldfish.jpg', 'ILSVRC2012_val_00001143.JPEG', 'ILSVRC2012_val_00001209.JPEG', 'ILSVRC2012_val_00003169.JPEG', 'ILSVRC2012_val_00005140.JPEG', 'ILSVRC2012_val_00006505.JPEG', 'ILSVRC2012_val_00007455.JPEG', 'ILSVRC2012_val_00010495.JPEG', 'ILSVRC2012_val_00012187.JPEG', 'ILSVRC2012_val_00021056.JPEG', 'ILSVRC2012_val_00021591.JPEG', 'ILSVRC2012_val_00021821.JPEG', 'ILSVRC2012_val_00022112.JPEG', 'ILSVRC2012_val_00024126.JPEG', 'ILSVRC2012_val_00026600.JPEG', 'ILSVRC2012_val_00027160 copy.JPEG', 'ILSVRC2012_val_00030127.JPEG', 'ILSVRC2012_val_00035705.JPEG', 'ILSVRC2012_val_00038873.JPEG', 'ILSVRC2012_val_00039797.JPEG', 'ILSVRC2012_val_00047413.JPEG', 'ILSVRC2012_val_00048952.JPEG']\n",
      "Found 24 images in the folder\n",
      "Image files: ['catdog.png', 'golden-retriever-puppy.jpg', 'goldfish.jpg', 'ILSVRC2012_val_00001143.JPEG', 'ILSVRC2012_val_00001209.JPEG', 'ILSVRC2012_val_00003169.JPEG', 'ILSVRC2012_val_00005140.JPEG', 'ILSVRC2012_val_00006505.JPEG', 'ILSVRC2012_val_00007455.JPEG', 'ILSVRC2012_val_00010495.JPEG', 'ILSVRC2012_val_00012187.JPEG', 'ILSVRC2012_val_00021056.JPEG', 'ILSVRC2012_val_00021591.JPEG', 'ILSVRC2012_val_00021821.JPEG', 'ILSVRC2012_val_00022112.JPEG', 'ILSVRC2012_val_00024126.JPEG', 'ILSVRC2012_val_00026600.JPEG', 'ILSVRC2012_val_00027160 copy.JPEG', 'ILSVRC2012_val_00030127.JPEG', 'ILSVRC2012_val_00035705.JPEG', 'ILSVRC2012_val_00038873.JPEG', 'ILSVRC2012_val_00039797.JPEG', 'ILSVRC2012_val_00047413.JPEG', 'ILSVRC2012_val_00048952.JPEG']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 24/24 [00:46<00:00,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metrics for 24 images:\n",
      "+----------------+------------+----------+------------+----------+----------------+\n",
      "| Saliency Map   |   Avg DAUC |   Avg DC |   Avg IAUC |   Avg IC |   Avg Sparsity |\n",
      "+================+============+==========+============+==========+================+\n",
      "| RISE           |     0.1068 |   0.2282 |     0.5785 |   0.0612 |         2.2170 |\n",
      "+----------------+------------+----------+------------+----------+----------------+\n",
      "Script completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Set up the path to your RISE-SFL folder\n",
    "rise_sfl_path = r\"C:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\Causal-AI\"\n",
    "sys.path.append(rise_sfl_path)\n",
    "\n",
    "# Assuming these imports work as in your original script\n",
    "from explanations import RISE\n",
    "from utils import read_tensor, get_class_name, tensor_imshow\n",
    "print(\"Script started\")\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "model = models.resnet50(True)\n",
    "model = nn.Sequential(model, nn.Softmax(dim=1))\n",
    "model = model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "print(\"Model loaded\")\n",
    "\n",
    "# Initialize RISE explainer\n",
    "explainer = RISE(model, (224, 224))\n",
    "N, s, p1 = 100, 8, 0.2\n",
    "explainer.generate_masks(N, s, p1)\n",
    "explainer.load_masks('masks.npy',p1,s)\n",
    "\n",
    "print(\"RISE explainer initialized\")\n",
    "\n",
    "# Process all images in the folder\n",
    "image_folder = r'C:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\RISE-SFL\\data_demo'\n",
    "\n",
    "print(f\"Checking image folder: {image_folder}\")\n",
    "print(f\"Folder exists: {os.path.exists(image_folder)}\")\n",
    "print(f\"Folder contents: {os.listdir(image_folder)}\")\n",
    "\n",
    "image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "print(f\"Found {len(image_files)} images in the folder\")\n",
    "print(f\"Image files: {image_files}\")\n",
    "\n",
    "# Function to calculate sparsity\n",
    "def calculate_sparsity(saliency_map): \n",
    "    max_value = np.max(saliency_map)\n",
    "    mean_value = np.mean(saliency_map)\n",
    "    return max_value / mean_value if mean_value != 0 else float('inf')\n",
    "\n",
    "rise_sfl_path = r\"C:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\metrics-saliency-maps\"  # Replace with the actual path to your RISE-SFL folder\n",
    "sys.path.append(rise_sfl_path)\n",
    "#saliency_maps = extract_specific_saliency_maps(pixel_datasets[0])\n",
    "\n",
    "from saliency_maps_metrics.multi_step_metrics import Deletion, Insertion, compute_correlation, compute_auc_metric\n",
    "from saliency_maps_metrics.data_replace import select_data_replace_method\n",
    "\n",
    "deletion_metric = Deletion(data_replace_method=\"black\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\n",
    "insertion_metric = Insertion(data_replace_method=\"blur\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\n",
    "\n",
    "\n",
    "# Initialize dictionary to store cumulative results\n",
    "cumulative_results = {\n",
    "    \"RISE\": {\"DAUC\": 0, \"DC\": 0, \"IAUC\": 0, \"IC\": 0, \"Sparsity\": 0}\n",
    "}\n",
    "\n",
    "# Process each image\n",
    "for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "    img_path = os.path.join(image_folder, image_file)\n",
    "    img = read_tensor(img_path).cuda()\n",
    "\n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "    target_class = output.argmax().item()\n",
    "\n",
    "    # Generate saliency map\n",
    "    sal_og = explainer(img)[target_class].cpu().numpy()\n",
    "\n",
    "    # Calculate metrics\n",
    "    saliency_map_tensor = torch.from_numpy(sal_og).float().unsqueeze(0).unsqueeze(0).cuda()\n",
    "\n",
    "    # Compute Deletion metrics\n",
    "    deletion_result = deletion_metric(model, img, saliency_map_tensor, class_to_explain_list=[target_class])\n",
    "    cumulative_results[\"RISE\"][\"DAUC\"] += deletion_result[\"dauc\"]\n",
    "    cumulative_results[\"RISE\"][\"DC\"] += deletion_result[\"dc\"]\n",
    "\n",
    "    # Compute Insertion metrics\n",
    "    insertion_result = insertion_metric(model, img, saliency_map_tensor, class_to_explain_list=[target_class])\n",
    "    cumulative_results[\"RISE\"][\"IAUC\"] += insertion_result[\"iauc\"]\n",
    "    cumulative_results[\"RISE\"][\"IC\"] += insertion_result[\"ic\"]\n",
    "\n",
    "    # Calculate sparsity\n",
    "    cumulative_results[\"RISE\"][\"Sparsity\"] += calculate_sparsity(sal_og)\n",
    "\n",
    "    # Visualize and save saliency map\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    tensor_imshow(img[0].cpu())\n",
    "    plt.title(f\"Original - {get_class_name(target_class)}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    tensor_imshow(img[0].cpu())\n",
    "    plt.imshow(sal_og, cmap='jet', alpha=0.5)\n",
    "    plt.title(\"Saliency Map\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.savefig(f\"saliency_{image_file}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Calculate averages\n",
    "num_images = len(image_files)\n",
    "results = []\n",
    "for name, metrics in cumulative_results.items():\n",
    "    avg_metrics = [name] + [metrics[key] / num_images for key in [\"DAUC\", \"DC\", \"IAUC\", \"IC\", \"Sparsity\"]]\n",
    "    results.append(avg_metrics)\n",
    "\n",
    "# Create a table using tabulate\n",
    "headers = [\"Saliency Map\", \"Avg DAUC\", \"Avg DC\", \"Avg IAUC\", \"Avg IC\", \"Avg Sparsity\"]\n",
    "table = tabulate(results, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\")\n",
    "\n",
    "# Print the table\n",
    "print(f\"\\nAverage Metrics for {num_images} images:\")\n",
    "print(table)\n",
    "\n",
    "print(\"Script completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "RISE explainer initialized\n",
      "Checking image folder: C:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\causal-explainer\\data_benchmark\n",
      "Folder exists: True\n",
      "Folder contents: ['.DS_Store', 'catdog.png', 'golden-retriever-puppy.jpg', 'goldfish.jpg', 'ILSVRC2012_val_00001209.JPEG', 'ILSVRC2012_val_00003169.JPEG', 'ILSVRC2012_val_00005140.JPEG', 'ILSVRC2012_val_00006505.JPEG', 'ILSVRC2012_val_00007455.JPEG', 'ILSVRC2012_val_00010495.JPEG', 'ILSVRC2012_val_00021056.JPEG', 'ILSVRC2012_val_00021591.JPEG', 'ILSVRC2012_val_00021821.JPEG', 'ILSVRC2012_val_00022112.JPEG', 'ILSVRC2012_val_00024126.JPEG', 'ILSVRC2012_val_00026600.JPEG', 'ILSVRC2012_val_00027160 copy.JPEG', 'ILSVRC2012_val_00030127.JPEG', 'ILSVRC2012_val_00035705.JPEG', 'ILSVRC2012_val_00038873.JPEG', 'ILSVRC2012_val_00039797.JPEG', 'ILSVRC2012_val_00047413.JPEG', 'ILSVRC2012_val_00048952.JPEG']\n",
      "Found 22 images in the folder\n",
      "Image files: ['catdog.png', 'golden-retriever-puppy.jpg', 'goldfish.jpg', 'ILSVRC2012_val_00001209.JPEG', 'ILSVRC2012_val_00003169.JPEG', 'ILSVRC2012_val_00005140.JPEG', 'ILSVRC2012_val_00006505.JPEG', 'ILSVRC2012_val_00007455.JPEG', 'ILSVRC2012_val_00010495.JPEG', 'ILSVRC2012_val_00021056.JPEG', 'ILSVRC2012_val_00021591.JPEG', 'ILSVRC2012_val_00021821.JPEG', 'ILSVRC2012_val_00022112.JPEG', 'ILSVRC2012_val_00024126.JPEG', 'ILSVRC2012_val_00026600.JPEG', 'ILSVRC2012_val_00027160 copy.JPEG', 'ILSVRC2012_val_00030127.JPEG', 'ILSVRC2012_val_00035705.JPEG', 'ILSVRC2012_val_00038873.JPEG', 'ILSVRC2012_val_00039797.JPEG', 'ILSVRC2012_val_00047413.JPEG', 'ILSVRC2012_val_00048952.JPEG']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/22 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [22, 200, 3, 224, 224]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 157\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[0;32m    156\u001b[0m image_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124meagua\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m6CCS3COV-7CCSMCVI-Computer-Vision\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcausal-explainer\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata_benchmark\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 157\u001b[0m \u001b[43manalyze_saliency_maps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manalysis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Set analysis=False to skip individual results\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 83\u001b[0m, in \u001b[0;36manalyze_saliency_maps\u001b[1;34m(image_folder, analysis)\u001b[0m\n\u001b[0;32m     80\u001b[0m target_class \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Generate saliency map\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m sal_og \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m[target_class]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[0;32m     86\u001b[0m saliency_map_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(sal_og)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\Causal-AI\\explanations.py:102\u001b[0m, in \u001b[0;36mRISE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    100\u001b[0m p \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, N, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_batch):\n\u001b[1;32m--> 102\u001b[0m     p\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    103\u001b[0m p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(p)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Number of classes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eagua\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [22, 200, 3, 224, 224]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up the path to your RISE-SFL folder\n",
    "rise_sfl_path = r\"C:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\Causal-AI\"\n",
    "sys.path.append(rise_sfl_path)\n",
    "\n",
    "from explanations import RISE\n",
    "from utils import read_tensor, get_class_name, tensor_imshow\n",
    "\n",
    "metrics_path = r\"C:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\metrics-saliency-maps\"\n",
    "sys.path.append(metrics_path)\n",
    "from saliency_maps_metrics.multi_step_metrics import Deletion, Insertion, compute_correlation, compute_auc_metric\n",
    "from saliency_maps_metrics.data_replace import select_data_replace_method\n",
    "\n",
    "print(\"Script started\")\n",
    "\n",
    "# Set up CUDA\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "def analyze_saliency_maps(image_folder, analysis=False):\n",
    "    # Load the model\n",
    "    model = models.resnet50(True)\n",
    "    model = nn.Sequential(model, nn.Softmax(dim=1))\n",
    "    model = model.eval()\n",
    "    model = model.cuda()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "\n",
    "    # Initialize RISE explainer\n",
    "    explainer = RISE(model, (224, 224))\n",
    "    N, s, p1 = 200, 8, 0.2\n",
    "    #explainer.generate_masks(N, s, p1)\n",
    "    explainer.load_masks('mask.npy', p1, s)\n",
    "\n",
    "    print(\"RISE explainer initialized\")\n",
    "\n",
    "    # Set up metrics\n",
    "    deletion_metric = Deletion(data_replace_method=\"black\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\n",
    "    insertion_metric = Insertion(data_replace_method=\"blur\", bound_max_step=True, batch_size=1, max_step_nb=14*14, cumulative=True)\n",
    "\n",
    "    print(f\"Checking image folder: {image_folder}\")\n",
    "    print(f\"Folder exists: {os.path.exists(image_folder)}\")\n",
    "    print(f\"Folder contents: {os.listdir(image_folder)}\")\n",
    "\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    print(f\"Found {len(image_files)} images in the folder\")\n",
    "    print(f\"Image files: {image_files}\")\n",
    "\n",
    "    # Function to calculate sparsity\n",
    "    def calculate_sparsity(saliency_map):\n",
    "        max_value = np.max(saliency_map)\n",
    "        mean_value = np.mean(saliency_map)\n",
    "        return max_value / mean_value if mean_value != 0 else float('inf')\n",
    "\n",
    "    # Initialize dictionary to store results for each image\n",
    "    image_results = {}\n",
    "\n",
    "    # Process each image\n",
    "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        img_path = os.path.join(image_folder, image_file)\n",
    "        img = read_tensor(img_path).cuda()\n",
    "\n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "        target_class = output.argmax().item()\n",
    "\n",
    "        # Generate saliency map\n",
    "        sal_og = explainer(img)[target_class].cpu().numpy()\n",
    "\n",
    "        # Calculate metrics\n",
    "        saliency_map_tensor = torch.from_numpy(sal_og).float().unsqueeze(0).unsqueeze(0).cuda()\n",
    "\n",
    "        # Compute Deletion metrics\n",
    "        deletion_result = deletion_metric(model, img, saliency_map_tensor, class_to_explain_list=[target_class])\n",
    "\n",
    "        # Compute Insertion metrics\n",
    "        insertion_result = insertion_metric(model, img, saliency_map_tensor, class_to_explain_list=[target_class])\n",
    "\n",
    "        # Calculate sparsity\n",
    "        sparsity = calculate_sparsity(sal_og)\n",
    "\n",
    "        # Store results for this image\n",
    "        image_results[image_file] = {\n",
    "            \"DAUC\": deletion_result[\"dauc\"],\n",
    "            \"DC\": deletion_result[\"dc\"],\n",
    "            \"IAUC\": insertion_result[\"iauc\"],\n",
    "            \"IC\": insertion_result[\"ic\"],\n",
    "            \"Sparsity\": sparsity\n",
    "        }\n",
    "\n",
    "        # Visualize and save saliency map\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        tensor_imshow(img[0].cpu())\n",
    "        plt.title(f\"Original - {get_class_name(target_class)}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        tensor_imshow(img[0].cpu())\n",
    "        plt.imshow(sal_og, cmap='jet', alpha=0.5)\n",
    "        plt.title(\"Saliency Map\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.savefig(f\"saliency_{image_file}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # Print individual results if analysis is True\n",
    "    if analysis:\n",
    "        for image_file, results in image_results.items():\n",
    "            print(f\"\\nMetrics for {image_file}:\")\n",
    "            \n",
    "            headers = [\"Metric\", \"Value\"]\n",
    "            table_data = [[key, f\"{value:.4f}\"] for key, value in results.items()]\n",
    "            \n",
    "            table = tabulate(table_data, headers=headers, tablefmt=\"grid\")\n",
    "            print(table)\n",
    "\n",
    "    # Calculate overall averages and standard deviations\n",
    "    overall_results = {key: [] for key in [\"DAUC\", \"DC\", \"IAUC\", \"IC\", \"Sparsity\"]}\n",
    "\n",
    "    for results in image_results.values():\n",
    "        for key, value in results.items():\n",
    "            overall_results[key].append(value)\n",
    "\n",
    "    # Create the overall summary table\n",
    "    print(\"\\nOverall Summary:\")\n",
    "    headers = [\"Metric\", \"Avg ± Std\"]\n",
    "    summary_data = []\n",
    "\n",
    "    for key, values in overall_results.items():\n",
    "        avg = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        summary_data.append([key, f\"{avg:.4f} ± {std:.4f}\"])\n",
    "\n",
    "    summary_table = tabulate(summary_data, headers=headers, tablefmt=\"grid\")\n",
    "    print(summary_table)\n",
    "\n",
    "    print(\"Analysis completed\")\n",
    "\n",
    "# Usage\n",
    "image_folder = r'C:\\Users\\eagua\\OneDrive\\Documents\\GitHub\\6CCS3COV-7CCSMCVI-Computer-Vision\\causal-explainer\\data_benchmark'\n",
    "analyze_saliency_maps(image_folder, analysis=True)  # Set analysis=False to skip individual results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
